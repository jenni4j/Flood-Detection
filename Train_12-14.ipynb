{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbu_ucRSo5zT"
   },
   "source": [
    "The following is an example of how to utilize our Sen1Floods11 dataset for training a FCNN. In this example, we train and validate on hand-labeled chips of flood events. However, our dataset includes several other options that are detailed in the README. To replace the dataset, as outlined further below, simply replace the train, test, and validation split csv's, and download the corresponding dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TQtMrI_VhKk"
   },
   "source": [
    "Authenticate Google Cloud Platform. Note that to run this code, you must connect your notebook runtime to a GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXGTA6vHVyJX"
   },
   "source": [
    "Install RasterIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NLlVutLzV_pZ"
   },
   "outputs": [],
   "source": [
    "#!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install scikit-learn\n",
    "#%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLqL9C2Rg6eB"
   },
   "source": [
    "Define a model checkpoint folder, for storing network checkpoints during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yLlIhE-Hg-Ym"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%cd /home\\n!sudo mkdir checkpoints'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%cd /home\n",
    "!sudo mkdir checkpoints\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwrDM4AjVnbU"
   },
   "source": [
    "Download train, test, and validation splits for both flood water. To download different train, test, and validation splits, simply replace these paths with the path to a csv containing the desired splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RFLsGwdRWuO4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!gsutil cp gs://sen1floods11/v1.1/splits/flood_handlabeled/flood_train_data.csv .\\n!gsutil cp gs://sen1floods11/v1.1/splits/flood_handlabeled/flood_test_data.csv .\\n!gsutil cp gs://sen1floods11/v1.1/splits/flood_handlabeled/flood_valid_data.csv .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!gsutil cp gs://sen1floods11/v1.1/splits/flood_handlabeled/flood_train_data.csv .\n",
    "!gsutil cp gs://sen1floods11/v1.1/splits/flood_handlabeled/flood_test_data.csv .\n",
    "!gsutil cp gs://sen1floods11/v1.1/splits/flood_handlabeled/flood_valid_data.csv .\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCAXpuKVW3eV"
   },
   "source": [
    "Download raw train, test, and validation data. In this example, we are downloading train, test, and validation data of flood images which are hand labeled. However, you can simply replace these paths with whichever dataset you would like to use - further documentation of the Sen1Floods11 dataset and organization is available in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ahAWnrSFW53S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!gsutil -m rsync -r gs://sen1floods11/v1.1/data/flood_events/HandLabeled/S1Hand files/S1\\n!gsutil -m rsync -r gs://sen1floods11/v1.1/data/flood_events/HandLabeled/LabelHand files/Labels'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"!gsutil -m rsync -r gs://sen1floods11/v1.1/data/flood_events/HandLabeled/S1Hand files/S1\n",
    "!gsutil -m rsync -r gs://sen1floods11/v1.1/data/flood_events/HandLabeled/LabelHand files/Labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_46CazV3XSCD"
   },
   "source": [
    "Define model training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fNYQywdWXeLM"
   },
   "outputs": [],
   "source": [
    "LR = 5e-4\n",
    "MAX_LR = 5e-4\n",
    "EPOCHS = 100\n",
    "\n",
    "EPOCHS_PER_UPDATE = 1\n",
    "RUNNAME = \"Sen1Floods11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9FJmTnZXjxj"
   },
   "source": [
    "Define functions to process and augment training and testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mBkfav0Eajqg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as fun\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "class InMemoryDataset(torch.utils.data.Dataset):\n",
    "  \n",
    "  def __init__(self, data_list, preprocess_func, source='S1', select_bands=(0,1,2)):\n",
    "    self.data_list = data_list\n",
    "    self.preprocess_func = preprocess_func\n",
    "    self.source = source\n",
    "    self.select_bands = select_bands\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    return self.preprocess_func(self.data_list[i], self.source, self.select_bands)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data_list)\n",
    "\n",
    "\n",
    "def processAndAugment(data, source='S1', select_bands=(0,1,2)): \n",
    "  (x,y) = data\n",
    "  im,label = x.copy(), y.copy()\n",
    "  label = label.astype(np.float)\n",
    "  \n",
    "  if source == 'S1':\n",
    "    bands = 2\n",
    "  else:\n",
    "    bands = len(select_bands)\n",
    "\n",
    "  # convert to PIL for easier transforms\n",
    "  ims = []\n",
    "  for i in range(bands):\n",
    "    ims.append(Image.fromarray(im[i]))\n",
    "\n",
    "  label = Image.fromarray(label.squeeze())      \n",
    "\n",
    "  # Get params for random transforms\n",
    "  i, j, h, w = transforms.RandomCrop.get_params(ims[0], (256, 256))\n",
    "\n",
    "\n",
    "  for i in range(bands):\n",
    "    ims[i] = F.crop(ims[i], i, j, h, w)\n",
    "  label = F.crop(label, i, j, h, w)\n",
    " \n",
    "  if random.random() > 0.5:\n",
    "    for i in range(bands):\n",
    "      ims[i] = F.hflip(ims[i])\n",
    "    label = F.hflip(label)\n",
    "\n",
    "  if random.random() > 0.5:\n",
    "    for i in range(bands):\n",
    "      ims[i] = F.vflip(ims[i])\n",
    "    label = F.vflip(label)\n",
    "\n",
    "  if random.random() > 0.75:\n",
    "    rotation = random.choice((90, 180, 270))\n",
    "    for i in range(bands): \n",
    "      ims[i] = F.rotate(ims[i], rotation)\n",
    "    label = F.rotate(label, rotation)\n",
    "  \n",
    "  \"\"\"if random.random() > 0.2:\n",
    "    for i in range(bands):\n",
    "      ims[i] = F.gaussian_blur(ims[i], 7)\"\"\"\n",
    "\n",
    "  # What does this do\n",
    "  if source == 'S1':\n",
    "    norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
    "  else: #TODO band selector\n",
    "    mean_list = np.array([0.16269160022432763, 0.13960347063125136, 0.13640611841716485, \n",
    "    0.1218228479188587, 0.14660729066303788, 0.23869029753700105, 0.284561256276994, 0.2622957968923778, \n",
    "    0.3077482214806557, 0.048687436781988974, 0.006377861007811543, 0.20306476302374007, 0.11791660722096743])\n",
    "    std_list = np.array([0.07001713384623806, 0.07390945268205054, 0.07352482387959473, 0.08649366949997794, \n",
    "    0.07768803358037298, 0.09213683430927469, 0.10843734609719749, 0.10226341800670553, 0.1196442553176325, \n",
    "    0.03366110543131479, 0.014399923282248634, 0.09808706134697646, 0.07646083655721092])\n",
    "    norm = transforms.Normalize(mean_list[np.array(select_bands)], std_list[np.array(select_bands)])\n",
    "    \n",
    "\n",
    "  blur = transforms.GaussianBlur(19, (.5,1.5))\n",
    "  t1 = transforms.ToPILImage()\n",
    "\n",
    "  ims_T = []\n",
    "  for i in range(bands):\n",
    "    ims_T.append(transforms.ToTensor()(ims[i]).squeeze())\n",
    "  \n",
    "  im = torch.stack(ims_T)\n",
    "  if random.random() > .8:\n",
    "    #BLURSAVE = random.randint(0,1000000000)\n",
    "    #ts = t1(im)\n",
    "    #ts.save('blurred/{}crisp.png'.format(BLURSAVE))\n",
    "    im = blur(im)\n",
    "    #te = t1(im)\n",
    "    #te.save('blurred/{}blur.png'.format( BLURSAVE))\n",
    "\n",
    "  im = norm(im)\n",
    "  \n",
    "  label = transforms.ToTensor()(label).squeeze()\n",
    "  if torch.sum(label.gt(.003) * label.lt(.004)):\n",
    "    label *= 255\n",
    "  label = label.round()\n",
    "\n",
    "  return im, label\n",
    "\n",
    "\n",
    "def processTestIm(data, source='S1', select_bands=(0,1,2)): \n",
    "  if source == 'S1':\n",
    "    bands = 2\n",
    "  else:\n",
    "    bands = len(select_bands)\n",
    "  \n",
    "  (x,y) = data\n",
    "  im,label = x.copy(), y.copy()\n",
    "  label = label.astype(np.float)\n",
    "  if source == 'S1':\n",
    "    norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
    "  else: #TODO band selector\n",
    "    mean_list = np.array([0.16269160022432763, 0.13960347063125136, 0.13640611841716485, \n",
    "    0.1218228479188587, 0.14660729066303788, 0.23869029753700105, 0.284561256276994, 0.2622957968923778, \n",
    "    0.3077482214806557, 0.048687436781988974, 0.006377861007811543, 0.20306476302374007, 0.11791660722096743])\n",
    "    std_list = np.array([0.07001713384623806, 0.07390945268205054, 0.07352482387959473, 0.08649366949997794, \n",
    "    0.07768803358037298, 0.09213683430927469, 0.10843734609719749, 0.10226341800670553, 0.1196442553176325, \n",
    "    0.03366110543131479, 0.014399923282248634, 0.09808706134697646, 0.07646083655721092])\n",
    "    norm = transforms.Normalize(mean_list[np.array(select_bands)], std_list[np.array(select_bands)])\n",
    "\n",
    "\n",
    "  # convert to PIL for easier transforms\n",
    "  im_c = []\n",
    "  for i in range(bands):\n",
    "    im_c.append(Image.fromarray(im[i]).resize((512,512)))\n",
    "\n",
    "  label = Image.fromarray(label.squeeze()).resize((512,512))\n",
    "\n",
    "  im_cs = []\n",
    "  for i in range(bands):\n",
    "    im_cs.append([F.crop(im_c[i], 0, 0, 256, 256), F.crop(im_c[i], 0, 256, 256, 256),\n",
    "            F.crop(im_c[i], 256, 0, 256, 256), F.crop(im_c[i], 256, 256, 256, 256)])\n",
    "  labels = [F.crop(label, 0, 0, 256, 256), F.crop(label, 0, 256, 256, 256),\n",
    "            F.crop(label, 256, 0, 256, 256), F.crop(label, 256, 256, 256, 256)]\n",
    "\n",
    "  ims = []\n",
    "  for i in range(4):\n",
    "    temp = []\n",
    "    for j in range(bands):\n",
    "      temp.append(transforms.ToTensor()(im_cs[j][i]).squeeze())\n",
    "    ims.append(torch.stack(temp))\n",
    "      \n",
    "  \n",
    "  ims = [norm(im) for im in ims]\n",
    "  ims = torch.stack(ims)\n",
    "  \n",
    "  labels = [(transforms.ToTensor()(label).squeeze()) for label in labels]\n",
    "  labels = torch.stack(labels)\n",
    "  \n",
    "  \n",
    "  if torch.sum(labels.gt(.003) * labels.lt(.004)):\n",
    "    labels *= 255\n",
    "  labels = labels.round()\n",
    "  \n",
    "  return ims, labels\n",
    "\n",
    "def save_images(image_tensor, label_tensor, count, i):\n",
    "    img = torch.argmax(image_tensor, dim=0)\n",
    "    img = img * 255\n",
    "    img = img.cpu()\n",
    "    img = img.numpy().astype('uint8')\n",
    "    lbl = torch.clone(label_tensor)\n",
    "    lbl = lbl.cpu()\n",
    "    lbl = label_tensor.numpy().astype('uint8')\n",
    "    lbl[lbl==255] = 0.\n",
    "    lbl = lbl * 255\n",
    "    \n",
    "    img = Image.fromarray(img)\n",
    "    lbl = Image.fromarray(lbl)\n",
    "    img.save('predictions/pred_{}_{}.png'.format(count, i))\n",
    "    lbl.save('predictions/label_{}_{}.png'.format(count, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzmZIRuoeAuJ"
   },
   "source": [
    "Load *flood water* train, test, and validation data from splits. In this example, this is the data we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rQUnYCIBeG21"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "def getArrFlood(fname):\n",
    "  x = rasterio.open(fname).read()\n",
    "  return x\n",
    "\n",
    "def download_flood_water_data_from_list(l, source, select_bands): #TODO band selector\n",
    "\n",
    "  i = 0\n",
    "  tot_nan = 0\n",
    "  tot_good = 0\n",
    "  flood_data = []\n",
    "  for (im_fname, mask_fname) in l:\n",
    "    if not os.path.exists(os.path.join(\"data/\", im_fname)):\n",
    "      print('No data for ', im_fname)\n",
    "      continue\n",
    "\n",
    "    temp_x = getArrFlood(os.path.join(\"data/\", im_fname))\n",
    "    #TODO band selector slice applicable bands here\n",
    "    if source == 'S1':\n",
    "      arr_x = np.nan_to_num(temp_x)\n",
    "    else:\n",
    "      arr_x = np.nan_to_num(temp_x)[select_bands,:,:]\n",
    "    arr_y = getArrFlood(os.path.join(\"data/\", mask_fname))\n",
    "    arr_y[arr_y == -1] = 255 \n",
    "    \n",
    "    if source == 'S1':\n",
    "      arr_x = np.clip(arr_x, -50, 1)\n",
    "      arr_x = (arr_x + 50) / 51\n",
    "    else:\n",
    "      arr_x = arr_x / 10000\n",
    "      \n",
    "    if i % 100 == 0:\n",
    "      print(im_fname, mask_fname)\n",
    "    i += 1\n",
    "    flood_data.append((arr_x,arr_y))\n",
    "  #print(flood_data)\n",
    "  return flood_data\n",
    "\n",
    "# Isaac note: Change the fname to be the path to the weakly labeled csv (S1_Weak_data_Otsu.csv or S2_Index_Label_Weak.csv)\n",
    "def load_flood_train_data(input_root, label_root, source='S1', select_bands=(0,1,2)):\n",
    "  fname = \"splits/flood_handlabeled/flood_train_data.csv\"\n",
    "  training_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      training_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "\n",
    "  return download_flood_water_data_from_list(training_files, source, select_bands)\n",
    "\n",
    "def load_flood_valid_data(input_root, label_root, source='S1', select_bands=(0,1,2)):\n",
    "  fname = \"splits/flood_handlabeled/flood_valid_data.csv\"\n",
    "  validation_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      validation_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "\n",
    "  return download_flood_water_data_from_list(validation_files, source, select_bands)\n",
    "\n",
    "def load_flood_test_data(input_root, label_root, source='S1', select_bands=(0,1,2)):\n",
    "  fname = \"splits/flood_handlabeled/flood_test_data.csv\"\n",
    "  testing_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      testing_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "  \n",
    "  return download_flood_water_data_from_list(testing_files, source, select_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFp9jrHYfOUh"
   },
   "source": [
    "Load training data and validation data. Note that here, we have chosen to train and validate our model on flood data. However, you can simply replace the load function call with one of the options defined above to load a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"temp_train = load_flood_train_data('flood_events/HandLabeled/S1Hand/', 'flood_events/HandLabeled/LabelHand/', source='S1')\\n#print(type(temp_train))\\nprint(np.shape(temp_train[0][0]))\\nprint(len(temp_train))\\nlist_train = []\\nlist_test = []\\nfor i in range(len(temp_train)):\\n    list_train.append(temp_train[i][0])\\n    list_test.append(temp_train[i][1])\\ntrain_array = np.array(list_train)\\ntest_array = np.array(list_test)\\nprint(train_array.shape)\\nprint(test_array.shape)\\nprint(test_array[0,0])\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"temp_train = load_flood_train_data('flood_events/HandLabeled/S1Hand/', 'flood_events/HandLabeled/LabelHand/', source='S1')\n",
    "#print(type(temp_train))\n",
    "print(np.shape(temp_train[0][0]))\n",
    "print(len(temp_train))\n",
    "list_train = []\n",
    "list_test = []\n",
    "for i in range(len(temp_train)):\n",
    "    list_train.append(temp_train[i][0])\n",
    "    list_test.append(temp_train[i][1])\n",
    "train_array = np.array(list_train)\n",
    "test_array = np.array(list_test)\n",
    "print(train_array.shape)\n",
    "print(test_array.shape)\n",
    "print(test_array[0,0])\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'means = []\\nstds = []\\nfor i in range(13):\\n    means.append(np.mean(train_array[:,i,:,:]))\\n    stds.append(np.std(train_array[:,i,:,:]))\\nprint(means)\\nprint(stds)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(np.count_nonzero(test_array==2))\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"means = []\n",
    "stds = []\n",
    "for i in range(13):\n",
    "    means.append(np.mean(train_array[:,i,:,:]))\n",
    "    stds.append(np.std(train_array[:,i,:,:]))\n",
    "print(means)\n",
    "print(stds)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZcqPlsjBffXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flood_events/HandLabeled/S1Hand/Ghana_103272_S1Hand.tif flood_events/HandLabeled/LabelHand/Ghana_103272_LabelHand.tif\n",
      "flood_events/HandLabeled/S1Hand/Pakistan_132143_S1Hand.tif flood_events/HandLabeled/LabelHand/Pakistan_132143_LabelHand.tif\n",
      "flood_events/HandLabeled/S1Hand/Sri-Lanka_916628_S1Hand.tif flood_events/HandLabeled/LabelHand/Sri-Lanka_916628_LabelHand.tif\n",
      "flood_events/HandLabeled/S1Hand/Ghana_5079_S1Hand.tif flood_events/HandLabeled/LabelHand/Ghana_5079_LabelHand.tif\n"
     ]
    }
   ],
   "source": [
    "# Isaac note: change the train data arguments here to be the path to the weakly labeled data. The S1Weak folder contains the actual\n",
    "# training data. S1OtsuLabelWeak and S2IndexLabelWeak are different label sets generated by different algorithms. Use whichever you want.\n",
    "SOURCE = 'S1'\n",
    "train_data = load_flood_train_data('flood_events/HandLabeled/S1Hand/', 'flood_events/HandLabeled/LabelHand/', source=SOURCE, select_bands=(np.arange(13)))\n",
    "train_dataset = InMemoryDataset(train_data, processAndAugment, source=SOURCE, select_bands=(np.arange(13)))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                  pin_memory=True, drop_last=False, timeout=0,\n",
    "                  worker_init_fn=None)\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "valid_data = load_flood_valid_data('flood_events/HandLabeled/S1Hand/', 'flood_events/HandLabeled/LabelHand/', source=SOURCE, select_bands=(np.arange(13))) \n",
    "valid_dataset = InMemoryDataset(valid_data, processTestIm, source=SOURCE, select_bands=(np.arange(13)))\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=4, shuffle=True, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: (torch.cat([a[0] for a in x], 0), torch.cat([a[1] for a in x], 0)),\n",
    "                  pin_memory=True, drop_last=False, timeout=0,\n",
    "                  worker_init_fn=None)\n",
    "valid_iter = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define differentiable loss functions. Assessment metrics defined below use argmax, making them non-differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target, smooth=1):\n",
    "        if output.shape[1] == 1:\n",
    "            output = fun.sigmoid(output)\n",
    "        else:\n",
    "            output = fun.softmax(output, dim=1)[:,1]\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        output = output.flatten()\n",
    "        target = target.flatten()\n",
    "        no_ignore = target.ne(255).cuda()\n",
    "        output = output.masked_select(no_ignore)\n",
    "        target = target.masked_select(no_ignore)\n",
    "        TP = torch.sum(output * target)\n",
    "        return 1 - ((2. * TP + smooth) / (output.sum() + target.sum() + smooth))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Loss squared with optional gamma\n",
    "class DiceLossSquared(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, gamma=1):\n",
    "        super(DiceLossSquared, self).__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, output, target, smooth=1):\n",
    "        if output.shape[1] == 1:\n",
    "            output = fun.sigmoid(output)\n",
    "        else:\n",
    "            output = fun.softmax(output, dim=1)[:,1]\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        output = output.flatten()\n",
    "        target = target.flatten()\n",
    "        no_ignore = target.ne(255).cuda()\n",
    "        output = output.masked_select(no_ignore)\n",
    "        target = target.masked_select(no_ignore)\n",
    "        TP = torch.sum(output * target)\n",
    "        return (1 - ((2. * TP + smooth) / ((output**2).sum() + (target**2).sum() + smooth)))**self.gamma\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOU(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, smooth=1):\n",
    "        super(IOU, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        if output.shape[1] == 1:\n",
    "            output = fun.sigmoid(output)\n",
    "        else:\n",
    "            output = fun.softmax(output, dim=1)[:,1]    \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        output = output.flatten()\n",
    "        target = target.flatten()\n",
    "        no_ignore = target.ne(255).cuda()\n",
    "        output = output.masked_select(no_ignore)\n",
    "        target = target.masked_select(no_ignore)\n",
    "        intersection = torch.sum(output * target)\n",
    "        union = torch.sum(target) + torch.sum(output) - intersection\n",
    "        return 1 - ((intersection + self.smooth) / (union + self.smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification to Dice loss to correct class imbalance\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, smooth=1, alpha = .5, beta = .5, gamma = 1):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        if output.shape[1] == 1:\n",
    "            output = fun.sigmoid(output)\n",
    "        else:\n",
    "            output = fun.softmax(output, dim=1)[:,1]       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        output = output.flatten()\n",
    "        target = target.flatten()\n",
    "        no_ignore = target.ne(255).cuda()\n",
    "        output = output.masked_select(no_ignore)\n",
    "        target = target.masked_select(no_ignore)\n",
    "        TP = torch.sum(output * target)\n",
    "        FP = torch.sum((1 - target) * output)\n",
    "        FN = torch.sum(target * (1 - output))\n",
    "        return (1 - ((TP + self.smooth) / (TP + (self.alpha * FN) + (self.beta * FP) + self.smooth)))**self.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3aAhUi2fp7M"
   },
   "source": [
    "Define the network. For our purposes, we use ResNet50. However, if you wish to test a different model framework, optimizer, or loss function you can simply replace those here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5cp4uXI1f9dr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gamad\\anaconda3\\envs\\cs7643-a2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gamad\\anaconda3\\envs\\cs7643-a2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\gamad\\anaconda3\\envs\\cs7643-a2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and will be removed in 0.15, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gamad\\anaconda3\\envs\\cs7643-a2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights_backbone=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from src import seg_models\n",
    "\n",
    "net = models.segmentation.fcn_resnet50(pretrained=False, num_classes=2, pretrained_backbone=False)\n",
    "#print('initial net:', net)\n",
    "net.backbone.conv1 = nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "#net = seg_models.MyUnet()\n",
    "#net = seg_models.AttentionUNet()\n",
    "#criterion = IOU()\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1,8]).float().cuda(), ignore_index=255) \n",
    "#criterion = DiceLoss()\n",
    "#criterion = DiceLossSquared()\n",
    "#criterion = FocalTverskyLoss()\n",
    "optimizer = torch.optim.AdamW(net.parameters(),lr=LR)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr= MAX_LR, momentum=.9)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, len(train_loader) * 10, T_mult=2, eta_min=0, last_epoch=-1)\n",
    "#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=LR, max_lr=MAX_LR, step_size_up=128)\n",
    "\n",
    "def convertBNtoGN(module, num_groups=16):\n",
    "  if isinstance(module, torch.nn.modules.batchnorm.BatchNorm2d):\n",
    "    return nn.GroupNorm(num_groups, module.num_features,\n",
    "                        eps=module.eps, affine=module.affine)\n",
    "    if module.affine:\n",
    "        mod.weight.data = module.weight.data.clone().detach()\n",
    "        mod.bias.data = module.bias.data.clone().detach()\n",
    "\n",
    "  for name, child in module.named_children():\n",
    "      module.add_module(name, convertBNtoGN(child, num_groups=num_groups))\n",
    "\n",
    "  return module\n",
    "\n",
    "net = convertBNtoGN(net)\n",
    "#print('modified net:', net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_Sy3ALGgQjf"
   },
   "source": [
    "Define assessment metrics. For our purposes, we use overall accuracy and mean intersection over union. However, we also include functions for calculating true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bwxC-fVBgUIb"
   },
   "outputs": [],
   "source": [
    "def computeIOU(output, target):\n",
    "  #TODO sigmoid support\n",
    "  if output.shape[1] == 1:\n",
    "    temp = -output\n",
    "    output = torch.stack((output, temp), dim=1)\n",
    "\n",
    "  output = torch.argmax(output, dim=1).flatten()  \n",
    "  target = target.flatten()\n",
    "  \n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  intersection = torch.sum(output * target)\n",
    "  union = torch.sum(target) + torch.sum(output) - intersection\n",
    "  iou = (intersection + .0000001) / (union + .0000001)\n",
    "  \n",
    "  if iou != iou:\n",
    "    print(\"failed, replacing with 0\")\n",
    "    iou = torch.tensor(0).float()\n",
    "  \n",
    "  return iou\n",
    "  \n",
    "def computeAccuracy(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  \n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  correct = torch.sum(output.eq(target))\n",
    "  \n",
    "  return correct.float() / len(target)\n",
    "\n",
    "def truePositives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def trueNegatives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 0)\n",
    "  target = (target == 0)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def falsePositives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 1)\n",
    "  target = (target == 0)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def falseNegatives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 0)\n",
    "  target = (target == 1)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lun5tGoYgjWX"
   },
   "source": [
    "Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DubsYZ8GgkxD"
   },
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "\n",
    "def train_loop(inputs, labels, net, optimizer, scheduler):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  \n",
    "  # zero the parameter gradients\n",
    "  optimizer.zero_grad()\n",
    "  net = net.cuda()\n",
    "  \n",
    "  # forward + backward + optimize\n",
    "  outputs = net(inputs.cuda())\n",
    "  loss = criterion(outputs[\"out\"], labels.long().cuda())\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  print(scheduler.get_last_lr())\n",
    "  scheduler.step()\n",
    "\n",
    "  running_loss += loss\n",
    "  running_iou += computeIOU(outputs[\"out\"], labels.cuda())\n",
    "  running_accuracy += computeAccuracy(outputs[\"out\"], labels.cuda())\n",
    "  running_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iM3Jz__hgshh"
   },
   "source": [
    "Define validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_GmVaoRvguic"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "def validation_loop(validation_data_loader, net):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  global max_valid_iou\n",
    "\n",
    "  global training_losses\n",
    "  global training_accuracies\n",
    "  global training_ious\n",
    "  global valid_losses\n",
    "  global valid_accuracies\n",
    "  global valid_ious\n",
    "\n",
    "  net = net.eval()\n",
    "  net = net.cuda()\n",
    "  count = 0\n",
    "  iou = 0\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "  with torch.no_grad():\n",
    "      for (images, labels) in validation_data_loader:\n",
    "          net = net.cuda()\n",
    "          outputs = net(images.cuda())\n",
    "          valid_loss = criterion(outputs[\"out\"], labels.long().cuda())\n",
    "          valid_iou = computeIOU(outputs[\"out\"], labels.cuda())\n",
    "          valid_accuracy = computeAccuracy(outputs[\"out\"], labels.cuda())\n",
    "          iou += valid_iou\n",
    "          loss += valid_loss\n",
    "          accuracy += valid_accuracy\n",
    "          count += 1\n",
    "\n",
    "  iou = iou / count\n",
    "  accuracy = accuracy / count\n",
    "\n",
    "  if iou > max_valid_iou:\n",
    "    max_valid_iou = iou\n",
    "    save_path = os.path.join(\"checkpoints\", \"{}_{}_{}.cp\".format(RUNNAME, i, iou.item()))\n",
    "    optim_save_path = os.path.join(\"checkpoints\", \"{}_{}_{}_{}.cp\".format(RUNNAME, i, iou.item(), 'optim'))\n",
    "    scheduler_save_path = os.path.join(\"checkpoints\", \"{}_{}_{}_{}.cp\".format(RUNNAME, i, iou.item(), 'sheduler'))\n",
    "    torch.save(net.state_dict(), save_path)\n",
    "    torch.save(optimizer.state_dict(), optim_save_path)\n",
    "    torch.save(scheduler.state_dict(), scheduler_save_path)\n",
    "    print(\"model saved at\", save_path)\n",
    "    \n",
    "\n",
    "  loss = loss / count\n",
    "  print(\"Training Loss:\", running_loss / running_count)\n",
    "  print(\"Training IOU:\", running_iou / running_count)\n",
    "  print(\"Training Accuracy:\", running_accuracy / running_count)\n",
    "  print(\"Validation Loss:\", loss)\n",
    "  print(\"Validation IOU:\", iou)\n",
    "  print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "  \"\"\"training_losses.append(running_loss / running_count)\n",
    "  training_accuracies.append(running_accuracy / running_count)\n",
    "  training_ious.append(running_iou / running_count)\n",
    "  valid_losses.append(loss)\n",
    "  valid_accuracies.append(accuracy)\n",
    "  valid_ious.append(iou)\"\"\"\n",
    "  training_losses.append(running_loss.detach().cpu() / running_count)\n",
    "  training_accuracies.append(running_accuracy.detach().cpu() / running_count)\n",
    "  training_ious.append(running_iou.detach().cpu() / running_count)\n",
    "  valid_losses.append(loss.detach().cpu())\n",
    "  valid_accuracies.append(accuracy.detach().cpu())\n",
    "  valid_ious.append(iou.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBMattYshiUj"
   },
   "source": [
    "Define testing loop (here, you can replace assessment metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mI_mhL_ehjot"
   },
   "outputs": [],
   "source": [
    "def test_loop(test_data_loader, net):\n",
    "  net = net.eval()\n",
    "  net = net.cuda()\n",
    "  count = 0\n",
    "  iou = 0\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "  with torch.no_grad():\n",
    "      for (images, labels) in tqdm(test_data_loader):\n",
    "          net = net.cuda()\n",
    "          outputs = net(images.cuda())\n",
    "          for i in range(outputs['out'].shape[0]):\n",
    "            save_images(outputs['out'][i], labels[i], count, i)\n",
    "\n",
    "          valid_loss = criterion(outputs[\"out\"], labels.long().cuda())\n",
    "          valid_iou = computeIOU(outputs[\"out\"], labels.cuda())\n",
    "          iou += valid_iou\n",
    "          accuracy += computeAccuracy(outputs[\"out\"], labels.cuda())\n",
    "          count += 1\n",
    "\n",
    "  iou = iou / count\n",
    "  print(\"Test IOU:\", iou)\n",
    "  print(\"Test Accuracy:\", accuracy / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy9Fii06h17Q"
   },
   "source": [
    "Define training and validation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NZuKVC6wh4Go"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "running_loss = 0\n",
    "running_iou = 0\n",
    "running_count = 0\n",
    "running_accuracy = 0\n",
    "\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "\n",
    "def train_epoch(net, optimizer, scheduler, train_iter):\n",
    "  for (inputs, labels) in tqdm(train_iter):\n",
    "    train_loop(inputs.cuda(), labels.cuda(), net.cuda(), optimizer, scheduler)\n",
    " \n",
    "\n",
    "def train_validation_loop(net, optimizer, scheduler, train_loader,\n",
    "                          valid_loader, num_epochs, cur_epoch):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  net = net.train()\n",
    "  running_loss = 0\n",
    "  running_iou = 0\n",
    "  running_count = 0\n",
    "  running_accuracy = 0\n",
    "  \n",
    "  for i in tqdm(range(num_epochs)):\n",
    "    train_iter = iter(train_loader)\n",
    "    train_epoch(net, optimizer, scheduler, train_iter)\n",
    "  clear_output()\n",
    "  \n",
    "  print(\"Current Epoch:\", cur_epoch)\n",
    "  validation_loop(iter(valid_loader), net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"device = torch.device('cuda')\\nsave_path = 'checkpoints/Sen1Floods11_1_0.5941653251647949.cp'\\noptim_save_path = 'checkpoints/Sen1Floods11_1_0.5941653251647949_optim.cp'\\nscheduler_save_path = 'checkpoints/Sen1Floods11_1_0.5941653251647949_sheduler.cp'\\nnet_checkpoint = torch.load(save_path)\\noptim_checkpoint = torch.load(optim_save_path)\\nschedule_checkpoint = torch.load(scheduler_save_path)\\nnet.load_state_dict(net_checkpoint)\\nnet.to(device=device)\\noptimizer.load_state_dict(optim_checkpoint)\\nscheduler.load_state_dict(schedule_checkpoint)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"device = torch.device('cuda')\n",
    "save_path = 'checkpoints/Sen1Floods11_1_0.5941653251647949.cp'\n",
    "optim_save_path = 'checkpoints/Sen1Floods11_1_0.5941653251647949_optim.cp'\n",
    "scheduler_save_path = 'checkpoints/Sen1Floods11_1_0.5941653251647949_sheduler.cp'\n",
    "net_checkpoint = torch.load(save_path)\n",
    "optim_checkpoint = torch.load(optim_save_path)\n",
    "schedule_checkpoint = torch.load(scheduler_save_path)\n",
    "net.load_state_dict(net_checkpoint)\n",
    "net.to(device=device)\n",
    "optimizer.load_state_dict(optim_checkpoint)\n",
    "scheduler.load_state_dict(schedule_checkpoint)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3I88aY5iAWD"
   },
   "source": [
    "Train model and assess metrics over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8MRpxUGWiDTu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nfrom IPython.display import display\\nimport matplotlib.pyplot as plt\\n\\nmax_valid_iou = 0\\nstart = 0\\n\\nepochs = []\\ntraining_losses = []\\ntraining_accuracies = []\\ntraining_ious = []\\nvalid_losses = []\\nvalid_accuracies = []\\nvalid_ious = []\\n\\nfor i in range(start, EPOCHS):\\n  train_validation_loop(net, optimizer, scheduler, train_loader, valid_loader, 1, i)\\n  epochs.append(i)\\n  x = epochs\\n  plt.plot(x, training_losses, label=\\'training losses\\')\\n  #plt.plot(x, training_accuracies, \\'tab:orange\\', label=\\'training accuracy\\')\\n  plt.plot(x, training_ious, \\'tab:purple\\', label=\\'training iou\\')\\n  plt.plot(x, valid_losses, label=\\'valid losses\\')\\n  #plt.plot(x, valid_accuracies, \\'tab:red\\',label=\\'valid accuracy\\')\\n  plt.plot(x, valid_ious, \\'tab:green\\',label=\\'valid iou\\')\\n  plt.legend(loc=\"upper left\")\\n\\n  display(plt.show())\\n\\n  print(\"max valid iou:\", max_valid_iou)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_valid_iou = 0\n",
    "start = 0\n",
    "\n",
    "epochs = []\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "for i in range(start, EPOCHS):\n",
    "  train_validation_loop(net, optimizer, scheduler, train_loader, valid_loader, 1, i)\n",
    "  epochs.append(i)\n",
    "  x = epochs\n",
    "  plt.plot(x, training_losses, label='training losses')\n",
    "  #plt.plot(x, training_accuracies, 'tab:orange', label='training accuracy')\n",
    "  plt.plot(x, training_ious, 'tab:purple', label='training iou')\n",
    "  plt.plot(x, valid_losses, label='valid losses')\n",
    "  #plt.plot(x, valid_accuracies, 'tab:red',label='valid accuracy')\n",
    "  plt.plot(x, valid_ious, 'tab:green',label='valid iou')\n",
    "  plt.legend(loc=\"upper left\")\n",
    "\n",
    "  display(plt.show())\n",
    "\n",
    "  print(\"max valid iou:\", max_valid_iou)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flood_events/HandLabeled/S1Hand/Ghana_313799_S1Hand.tif flood_events/HandLabeled/LabelHand/Ghana_313799_LabelHand.tif\n"
     ]
    }
   ],
   "source": [
    "SOURCE = 'S1'\n",
    "test_data = load_flood_test_data('flood_events/HandLabeled/S1Hand/', 'flood_events/HandLabeled/LabelHand/', source=SOURCE, select_bands=(np.arange(13))) \n",
    "test_dataset = InMemoryDataset(test_data, processTestIm, source=SOURCE, select_bands=(np.arange(13)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: (torch.cat([a[0] for a in x], 0), torch.cat([a[1] for a in x], 0)),\n",
    "                  pin_memory=True, drop_last=False, timeout=0,\n",
    "                  worker_init_fn=None)\n",
    "test_iter = iter(test_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loach checkpoints and test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb46a61e220e4226acecb750fa8f321d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gamad\\AppData\\Local\\Temp\\ipykernel_35256\\4039013114.py:116: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label = label.astype(np.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test IOU: tensor(0.5018, device='cuda:0')\n",
      "Test Accuracy: tensor(0.9344, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "#save_path = 'checkpoints/Sen1Floods11_62_0.6036016941070557.cp' #S2 weak\n",
    "#save_path = 'checkpoints/Sen1Floods11_1347_0.6440591216087341.cp' \n",
    "#save_path = 'checkpoints/Default_53_0.5246185064315796.cp'\n",
    "save_path = 'checkpoints/S1WeakOtsu_15_0.5288943648338318.cp'\n",
    "#save_path = 'checkpoints/Baseline_S1_Dice_26_0.5623193383216858.cp'\n",
    "\n",
    "net_checkpoint = torch.load(save_path)\n",
    "net.load_state_dict(net_checkpoint)\n",
    "net.to(device=device)\n",
    "net.eval()\n",
    "\n",
    "test_loop(test_loader, net)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c180ce7c9e84699bf04c600cd945decd2da2ce3b3f38b74d7de213322d95cf6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
