{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d0d32e3-0a75-4835-a12b-879bb8edfd4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:35.443941Z",
     "iopub.status.busy": "2022-12-09T20:39:35.443406Z",
     "iopub.status.idle": "2022-12-09T20:39:35.450500Z",
     "shell.execute_reply": "2022-12-09T20:39:35.448693Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.443872Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 0: Links of code used <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "\n",
    "# https://hsg-aiml.github.io/2022/06/16/Self_Supervised_Vision_Transformers_for_Land_cover_Segmentation_and_Classification.html\n",
    "# https://github.com/HSG-AIML/SSLTransformerRS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54e809b4-1edc-45a3-9bd6-f593012f6f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:35.453494Z",
     "iopub.status.busy": "2022-12-09T20:39:35.453117Z",
     "iopub.status.idle": "2022-12-09T20:39:35.462620Z",
     "shell.execute_reply": "2022-12-09T20:39:35.460239Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.453467Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1090859972.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [33]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install -r requirements.txt\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 1: Ensure requirements are installed <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17c9ff-b206-428e-9364-36f6df27b251",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T20:39:35.463938Z",
     "iopub.status.idle": "2022-12-09T20:39:35.464525Z",
     "shell.execute_reply": "2022-12-09T20:39:35.464251Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.464233Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 2: Import packages <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "\n",
    "# Import needed packages\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from distutils.util import strtobool\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18, resnet50\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41828e-ff0d-4cf8-817e-29569c09946e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T20:39:35.465712Z",
     "iopub.status.idle": "2022-12-09T20:39:35.466242Z",
     "shell.execute_reply": "2022-12-09T20:39:35.466005Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.465982Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 3: Data configurations <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "\n",
    "data_config = {\n",
    "    'num_classes': 2, # number of classes in the dataset.\n",
    "    'clip_sample_values': True, # clip (limit) values\n",
    "    'train_used_data_fraction': 1, # fraction of data to use, should be in the range [0, 1]\n",
    "    'val_used_data_fraction': 1,\n",
    "    'image_px_size': 224, # image size (224x224)\n",
    "    'cover_all_parts_train': True, # if True, if image_px_size is not 224 during training, we use a random crop of the image\n",
    "    'cover_all_parts_validation': True, # if True, if image_px_size is not 224 during validation, we use a non-overlapping sliding window to cover the entire image\n",
    "    'seed': 42,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df872dd4-33b3-467d-865c-b3e453d8c75f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T20:39:35.468424Z",
     "iopub.status.idle": "2022-12-09T20:39:35.468937Z",
     "shell.execute_reply": "2022-12-09T20:39:35.468714Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.468687Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 4: Ensure deterministic behavior <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specfic to Transformer\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "random.seed(data_config['seed'])\n",
    "np.random.seed(data_config['seed'])\n",
    "torch.manual_seed(data_config['seed'])\n",
    "torch.cuda.manual_seed_all(data_config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb481db-8ae0-422c-9f7b-9ca6189f0001",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-12-09T20:39:35.470437Z",
     "iopub.status.idle": "2022-12-09T20:39:35.470898Z",
     "shell.execute_reply": "2022-12-09T20:39:35.470727Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.470707Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 5: Functions of data download <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Slightly specific to Transformer - only change is I added a variable called im_size, which is set to 224x224 in order for the Transformer pretraining code to work\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as fun\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class InMemoryDataset(torch.utils.data.Dataset):\n",
    "  \n",
    "  def __init__(self, data_list, preprocess_func, source='S1', select_bands=(0,1,2), im_size=224):\n",
    "    self.data_list = data_list\n",
    "    self.preprocess_func = preprocess_func\n",
    "    self.source = source\n",
    "    self.select_bands = select_bands\n",
    "    self.im_size = im_size\n",
    "  \n",
    "  def __getitem__(self, i):\n",
    "    return self.preprocess_func(self.data_list[i], self.source, self.select_bands)\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data_list)\n",
    "\n",
    "\n",
    "def processAndAugment(data, source='S1', select_bands=(0,1,2), im_size=224): \n",
    "  (x,y) = data\n",
    "  im,label = x.copy(), y.copy()\n",
    "  label = label.astype(np.float)\n",
    "  \n",
    "  if source == 'S1':\n",
    "    bands = 2\n",
    "  else:\n",
    "    bands = len(select_bands)\n",
    "\n",
    "  # convert to PIL for easier transforms\n",
    "  ims = []\n",
    "  for i in range(bands):\n",
    "    ims.append(Image.fromarray(im[i]))\n",
    "\n",
    "  label = Image.fromarray(label.squeeze())      \n",
    "\n",
    "  # Get params for random transforms\n",
    "  i, j, h, w = transforms.RandomCrop.get_params(ims[0], (im_size, im_size))\n",
    "\n",
    "\n",
    "  for i in range(bands):\n",
    "    ims[i] = F.crop(ims[i], i, j, h, w)\n",
    "  label = F.crop(label, i, j, h, w)\n",
    " \n",
    "  if random.random() > 0.5:\n",
    "    for i in range(bands):\n",
    "      ims[i] = F.hflip(ims[i])\n",
    "    label = F.hflip(label)\n",
    "\n",
    "  if random.random() > 0.5:\n",
    "    for i in range(bands):\n",
    "      ims[i] = F.vflip(ims[i])\n",
    "    label = F.vflip(label)\n",
    "\n",
    "  if random.random() > 0.75:\n",
    "    rotation = random.choice((90, 180, 270))\n",
    "    for i in range(bands): \n",
    "      ims[i] = F.rotate(ims[i], rotation)\n",
    "    label = F.rotate(label, rotation)\n",
    "  \n",
    "  \"\"\"if random.random() > 0.2:\n",
    "    for i in range(bands):\n",
    "      ims[i] = F.gaussian_blur(ims[i], 7)\"\"\"\n",
    "\n",
    "  # What does this do\n",
    "  if source == 'S1':\n",
    "    norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
    "  else: #TODO band selector\n",
    "    mean_list = np.array([0.16269160022432763, 0.13960347063125136, 0.13640611841716485, \n",
    "    0.1218228479188587, 0.14660729066303788, 0.23869029753700105, 0.284561256276994, 0.2622957968923778, \n",
    "    0.3077482214806557, 0.048687436781988974, 0.006377861007811543, 0.20306476302374007, 0.11791660722096743])\n",
    "    std_list = np.array([0.07001713384623806, 0.07390945268205054, 0.07352482387959473, 0.08649366949997794, \n",
    "    0.07768803358037298, 0.09213683430927469, 0.10843734609719749, 0.10226341800670553, 0.1196442553176325, \n",
    "    0.03366110543131479, 0.014399923282248634, 0.09808706134697646, 0.07646083655721092])\n",
    "    norm = transforms.Normalize(mean_list[np.array(select_bands)], std_list[np.array(select_bands)])\n",
    "    \n",
    "\n",
    "  blur = transforms.GaussianBlur(7)\n",
    "\n",
    "  ims_T = []\n",
    "  for i in range(bands):\n",
    "    ims_T.append(transforms.ToTensor()(ims[i]).squeeze())\n",
    "  \n",
    "  im = torch.stack(ims_T)\n",
    "  if random.random() > .2:\n",
    "    im = blur(im)\n",
    "  im = norm(im)\n",
    "  \n",
    "  label = transforms.ToTensor()(label).squeeze()\n",
    "  if torch.sum(label.gt(.003) * label.lt(.004)):\n",
    "    label *= 255\n",
    "  label = label.round()\n",
    "\n",
    "  return im, label\n",
    "\n",
    "\n",
    "def processTestIm(data, source='S1', select_bands=(0,1,2), im_size=224): \n",
    "  if source == 'S1':\n",
    "    bands = 2\n",
    "  else:\n",
    "    bands = len(select_bands)\n",
    "  \n",
    "  (x,y) = data\n",
    "  im,label = x.copy(), y.copy()\n",
    "  label = label.astype(np.float)\n",
    "  if source == 'S1':\n",
    "    norm = transforms.Normalize([0.6851, 0.5235], [0.0820, 0.1102])\n",
    "  else: #TODO band selector\n",
    "    mean_list = np.array([0.16269160022432763, 0.13960347063125136, 0.13640611841716485, \n",
    "    0.1218228479188587, 0.14660729066303788, 0.23869029753700105, 0.284561256276994, 0.2622957968923778, \n",
    "    0.3077482214806557, 0.048687436781988974, 0.006377861007811543, 0.20306476302374007, 0.11791660722096743])\n",
    "    std_list = np.array([0.07001713384623806, 0.07390945268205054, 0.07352482387959473, 0.08649366949997794, \n",
    "    0.07768803358037298, 0.09213683430927469, 0.10843734609719749, 0.10226341800670553, 0.1196442553176325, \n",
    "    0.03366110543131479, 0.014399923282248634, 0.09808706134697646, 0.07646083655721092])\n",
    "    norm = transforms.Normalize(mean_list[np.array(select_bands)], std_list[np.array(select_bands)])\n",
    "\n",
    "\n",
    "  # convert to PIL for easier transforms\n",
    "  im_c = []\n",
    "  for i in range(bands):\n",
    "    im_c.append(Image.fromarray(im[i]).resize((512,512)))\n",
    "\n",
    "  label = Image.fromarray(label.squeeze()).resize((512,512))\n",
    "\n",
    "  im_cs = []\n",
    "  for i in range(bands):\n",
    "    im_cs.append([F.crop(im_c[i], 0, 0, im_size, im_size), F.crop(im_c[i], 0, im_size, im_size, im_size),\n",
    "            F.crop(im_c[i], im_size, 0, im_size, im_size), F.crop(im_c[i], im_size, im_size, im_size, im_size)])\n",
    "  labels = [F.crop(label, 0, 0, im_size, im_size), F.crop(label, 0, im_size, im_size, im_size),\n",
    "            F.crop(label, im_size, 0, im_size, im_size), F.crop(label, im_size, im_size, im_size, im_size)]\n",
    "\n",
    "  ims = []\n",
    "  for i in range(4):\n",
    "    temp = []\n",
    "    for j in range(bands):\n",
    "      temp.append(transforms.ToTensor()(im_cs[j][i]).squeeze())\n",
    "    ims.append(torch.stack(temp))\n",
    "      \n",
    "  \n",
    "  ims = [norm(im) for im in ims]\n",
    "  ims = torch.stack(ims)\n",
    "  \n",
    "  labels = [(transforms.ToTensor()(label).squeeze()) for label in labels]\n",
    "  labels = torch.stack(labels)\n",
    "  \n",
    "  \n",
    "  if torch.sum(labels.gt(.003) * labels.lt(.004)):\n",
    "    labels *= 255\n",
    "  labels = labels.round()\n",
    "  \n",
    "  return ims, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a16251a5-7eb0-4fdf-90a9-0aa947b35ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:35.521500Z",
     "iopub.status.busy": "2022-12-09T20:39:35.521134Z",
     "iopub.status.idle": "2022-12-09T20:39:35.537470Z",
     "shell.execute_reply": "2022-12-09T20:39:35.535675Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.521467Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 6: Define data download functions <<<<<<<<<<<<<<<<<<<<<<< \n",
    "# Not specific to Transformer\n",
    "\n",
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "def getArrFlood(fname):\n",
    "  x = rasterio.open(fname).read()\n",
    "  return x\n",
    "\n",
    "def download_flood_water_data_from_list(l, source, select_bands): #TODO band selector\n",
    "\n",
    "  i = 0\n",
    "  tot_nan = 0\n",
    "  tot_good = 0\n",
    "  flood_data = []\n",
    "  for (im_fname, mask_fname) in l:\n",
    "    # if not os.path.exists(os.path.join(\"data/\", im_fname)):\n",
    "      # print('No data for ', im_fname)\n",
    "      # continue\n",
    "\n",
    "    # temp_x = getArrFlood(os.path.join(\"data/\", im_fname))\n",
    "    temp_x = getArrFlood(im_fname)\n",
    "    #TODO band selector slice applicable bands here\n",
    "    if source == 'S1':\n",
    "      arr_x = np.nan_to_num(temp_x)\n",
    "    else:\n",
    "      arr_x = np.nan_to_num(temp_x)[select_bands,:,:]\n",
    "    # arr_y = getArrFlood(os.path.join(\"data/\", mask_fname))\n",
    "    arr_y = getArrFlood(mask_fname)\n",
    "    arr_y[arr_y == -1] = 255 \n",
    "    \n",
    "    if source == 'S1':\n",
    "      arr_x = np.clip(arr_x, -50, 1)\n",
    "      arr_x = (arr_x + 50) / 51\n",
    "    else:\n",
    "      arr_x = arr_x / 10000\n",
    "      \n",
    "    if i % 100 == 0:\n",
    "      print(im_fname, mask_fname)\n",
    "    i += 1\n",
    "    flood_data.append((arr_x,arr_y))\n",
    "  #print(flood_data)\n",
    "  return flood_data\n",
    "\n",
    "# Isaac note: Change the fname to be the path to the weakly labeled csv (S1_Weak_data_Otsu.csv or S2_Index_Label_Weak.csv)\n",
    "def load_flood_train_data(input_root, label_root, source='S1', select_bands=(0,1,2)):\n",
    "  # fname = \"splits/flood_handlabeled/flood_train_data.csv\"\n",
    "  fname = \"splits/flood_train_data.csv\"\n",
    "  training_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      # training_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "      training_files.append(tuple((input_root+'/'+line[0], label_root+'/'+line[1])))\n",
    "\n",
    "\n",
    "  return download_flood_water_data_from_list(training_files, source, select_bands)\n",
    "\n",
    "def load_flood_valid_data(input_root, label_root, source='S1', select_bands=(0,1,2)):\n",
    "  # fname = \"splits/flood_handlabeled/flood_valid_data.csv\"\n",
    "  fname = \"splits/flood_valid_data.csv\"\n",
    "  validation_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      # validation_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "      validation_files.append(tuple((input_root+'/'+line[0], label_root+'/'+line[1])))\n",
    "\n",
    "  return download_flood_water_data_from_list(validation_files, source, select_bands)\n",
    "\n",
    "def load_flood_test_data(input_root, label_root, source='S1', select_bands=(0,1,2)):\n",
    "  # fname = \"flood_test_data.csv\"\n",
    "  fname = \"splits/flood_test_data.csv\"\n",
    "  testing_files = []\n",
    "  with open(fname) as f:\n",
    "    for line in csv.reader(f):\n",
    "      # testing_files.append(tuple((input_root+line[0], label_root+line[1])))\n",
    "      testing_files.append(tuple((input_root+'/'+line[0], label_root+'/'+line[1])))\n",
    "  \n",
    "  return download_flood_water_data_from_list(testing_files, source, select_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af37f25c-710c-400b-9e12-ef2071b126df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:35.540942Z",
     "iopub.status.busy": "2022-12-09T20:39:35.540395Z",
     "iopub.status.idle": "2022-12-09T20:39:35.550746Z",
     "shell.execute_reply": "2022-12-09T20:39:35.548263Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.540844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a48489be-2aa9-418e-bd3e-83fd51a5696e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:35.552703Z",
     "iopub.status.busy": "2022-12-09T20:39:35.552251Z",
     "iopub.status.idle": "2022-12-09T20:39:35.560958Z",
     "shell.execute_reply": "2022-12-09T20:39:35.559668Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.552678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets\n"
     ]
    }
   ],
   "source": [
    "cd datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2735cb84-4f26-4b35-9716-5f5d274a8b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:35.563732Z",
     "iopub.status.busy": "2022-12-09T20:39:35.563236Z",
     "iopub.status.idle": "2022-12-09T20:39:36.363829Z",
     "shell.execute_reply": "2022-12-09T20:39:36.362348Z",
     "shell.execute_reply.started": "2022-12-09T20:39:35.563732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mflood_events\u001b[0m/  \u001b[01;34mlabelhand\u001b[0m/  \u001b[01;34mpretrain\u001b[0m/  \u001b[01;34ms1hand\u001b[0m/  \u001b[01;34ms2hand\u001b[0m/  \u001b[01;34msplits\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36f48204-f259-42ba-b204-ea2f0fd4c268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:36.365837Z",
     "iopub.status.busy": "2022-12-09T20:39:36.365548Z",
     "iopub.status.idle": "2022-12-09T20:39:47.422364Z",
     "shell.execute_reply": "2022-12-09T20:39:47.421190Z",
     "shell.execute_reply.started": "2022-12-09T20:39:36.365827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1hand/Ghana_103272_S1Hand.tif labelhand/Ghana_103272_LabelHand.tif\n",
      "s1hand/Pakistan_132143_S1Hand.tif labelhand/Pakistan_132143_LabelHand.tif\n",
      "s1hand/Sri-Lanka_916628_S1Hand.tif labelhand/Sri-Lanka_916628_LabelHand.tif\n",
      "s1hand/Ghana_5079_S1Hand.tif labelhand/Ghana_5079_LabelHand.tif\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 7: Perform data download <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "# Isaac note: change the train data arguments here to be the path to the weakly labeled data. The S1Weak folder contains the actual\n",
    "# training data. S1OtsuLabelWeak and S2IndexLabelWeak are different label sets generated by different algorithms. Use whichever you want.\n",
    "SOURCE = 'S1'\n",
    "# train_data = load_flood_train_data('flood_events/HandLabeled/S1Hand/', 'flood_events/HandLabeled/LabelHand/', source=SOURCE, select_bands=(np.arange(13)))\n",
    "train_data = load_flood_train_data('s1hand', 'labelhand', source=SOURCE, select_bands=(np.arange(13)))\n",
    "train_dataset = InMemoryDataset(train_data, processAndAugment, source=SOURCE, select_bands=(np.arange(13)), im_size=224)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                  pin_memory=True, drop_last=False, timeout=0,\n",
    "                  worker_init_fn=None)\n",
    "train_iter = iter(train_loader)\n",
    "\n",
    "# valid_data = load_flood_valid_data('flood_events/HandLabeled/S1Hand/', 'flood_events/HandLabeled/LabelHand/', source=SOURCE, select_bands=(np.arange(13))) \n",
    "valid_data = load_flood_valid_data('s1hand', 'labelhand', source=SOURCE, select_bands=(np.arange(13))) \n",
    "valid_dataset = InMemoryDataset(valid_data, processTestIm, source=SOURCE, select_bands=(np.arange(13)), im_size=224)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=4, shuffle=True, sampler=None,\n",
    "                  batch_sampler=None, num_workers=0, collate_fn=lambda x: (torch.cat([a[0] for a in x], 0), torch.cat([a[1] for a in x], 0)),\n",
    "                  pin_memory=True, drop_last=False, timeout=0,\n",
    "                  worker_init_fn=None)\n",
    "valid_iter = iter(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e13f5e5-d16c-490b-9d0d-102c6e46ddc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:47.425215Z",
     "iopub.status.busy": "2022-12-09T20:39:47.424512Z",
     "iopub.status.idle": "2022-12-09T20:39:47.433806Z",
     "shell.execute_reply": "2022-12-09T20:39:47.431803Z",
     "shell.execute_reply.started": "2022-12-09T20:39:47.425215Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 8: Training Configurations <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "\n",
    "train_config = {\n",
    "    's1_input_channels': 2,\n",
    "    's2_input_channels': 13,\n",
    "    'finetuning': False, # If false, backbone layers is frozen and only the head is trained\n",
    "    'classifier_lr': 3e-6,\n",
    "    'learning_rate': 0.1,\n",
    "    'adam_betas': (0.9, 0.999), \n",
    "    'weight_decay': 0.001,\n",
    "    'dataloader_workers': 4,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 10, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1886f49-dbc4-4a34-b04b-b9dec073fac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:47.436239Z",
     "iopub.status.busy": "2022-12-09T20:39:47.435979Z",
     "iopub.status.idle": "2022-12-09T20:39:47.734935Z",
     "shell.execute_reply": "2022-12-09T20:39:47.733627Z",
     "shell.execute_reply.started": "2022-12-09T20:39:47.436239Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 9: Load checkpoints & pre-trained model weights <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "\n",
    "# path to the checkpoint\n",
    "checkpoint = torch.load(\n",
    "    \"checkpoints/swin_t.pth\"\n",
    ") \n",
    "weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "# Sentinel-1 stream weights\n",
    "s1_weights = {\n",
    "    k[len(\"backbone1.\"):]: v for k, v in weights.items() if \"backbone1\" in k\n",
    "}\n",
    "\n",
    "# Sentinel-2 stream weights\n",
    "s2_weights = {\n",
    "    k[len(\"backbone2.\"):]: v for k, v in weights.items() if \"backbone2\" in k\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a8e660f-89b3-42df-9b23-a928a492e87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:47.737179Z",
     "iopub.status.busy": "2022-12-09T20:39:47.736613Z",
     "iopub.status.idle": "2022-12-09T20:39:47.745015Z",
     "shell.execute_reply": "2022-12-09T20:39:47.743352Z",
     "shell.execute_reply.started": "2022-12-09T20:39:47.736888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d06d12a-db1e-4735-bf16-f189b9159bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:47.747416Z",
     "iopub.status.busy": "2022-12-09T20:39:47.747007Z",
     "iopub.status.idle": "2022-12-09T20:39:47.755160Z",
     "shell.execute_reply": "2022-12-09T20:39:47.753432Z",
     "shell.execute_reply.started": "2022-12-09T20:39:47.747373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36533d35-e434-46b1-b5ca-3c0c5bb69e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:47.760180Z",
     "iopub.status.busy": "2022-12-09T20:39:47.759579Z",
     "iopub.status.idle": "2022-12-09T20:39:49.978358Z",
     "shell.execute_reply": "2022-12-09T20:39:49.976920Z",
     "shell.execute_reply.started": "2022-12-09T20:39:47.760142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 10: Load S1 & S2 backbones <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "\n",
    "from swin_transformer import DoubleSwinTransformerDownstream\n",
    "from utils import save_checkpoint_single_model, dotdictify\n",
    "from build import build_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu:0\")\n",
    "    \n",
    "input_channels = train_config['s1_input_channels']\n",
    "\n",
    "with open(\"backbone_config.json\", \"r\") as fp:\n",
    "    swin_conf = dotdictify(json.load(fp))\n",
    "    \n",
    "s1_backbone = build_model(swin_conf.model_config)\n",
    "\n",
    "swin_conf.model_config.MODEL.SWIN.IN_CHANS = 13\n",
    "\n",
    "s2_backbone = build_model(swin_conf.model_config)\n",
    "\n",
    "s1_backbone.load_state_dict(s1_weights)\n",
    "s2_backbone.load_state_dict(s2_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56cd3ffb-228d-4d23-9b68-7166f1898393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:49.980602Z",
     "iopub.status.busy": "2022-12-09T20:39:49.980224Z",
     "iopub.status.idle": "2022-12-09T20:39:49.996962Z",
     "shell.execute_reply": "2022-12-09T20:39:49.995176Z",
     "shell.execute_reply.started": "2022-12-09T20:39:49.980564Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 11: Segmentation Transformer Function <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "# Set up to use S1 input data -> If want S2, follow the below comments\n",
    "\n",
    "\n",
    "from swin_transformer import SwinTransformerDecoder\n",
    "\n",
    "\n",
    "class SingleSwinTransformerSegmentation(nn.Module):\n",
    "    # replace encoder1 with encoder2 for S2\n",
    "    def __init__(self, encoder1, out_dim, device, freeze_layers=True):\n",
    "        super(SingleSwinTransformerSegmentation, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # comment out backbone1 and uncomment backbone2 for S2\n",
    "        self.backbone1 = encoder1\n",
    "        # self.backbone2 = encoder2\n",
    "\n",
    "        # comment out decoder1 and uncomment decoder2 for S2\n",
    "        self.decoder1 = SwinTransformerDecoder(self.backbone1, out_dim, device)\n",
    "        # self.decoder2 = SwinTransformerDecoder(self.backbone2, out_dim, device)\n",
    "\n",
    "        # freeze all backbone layers\n",
    "        if freeze_layers:\n",
    "            for name, param in self.named_parameters():\n",
    "                if name.startswith(('backbone')):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # comment out x1 and uncomment x2 for S2\n",
    "        # _, x2, x_seg2 = self.backbone2.forward_features(x.to(self.device))\n",
    "        _, x1, x_seg1 = self.backbone1.forward_features(x.to(self.device))\n",
    "\n",
    "        # comment out x1 and uncomment x2 for S2\n",
    "        x1 = self.decoder1.forward_up_features(x1, x_seg1)\n",
    "        # x2 = self.decoder2.forward_up_features(x2, x_seg2)\n",
    "\n",
    "        # replace decoder1 with decoder2 and x1 with x2 for S2        \n",
    "        x = self.decoder1.up_x4(x1)\n",
    "        \n",
    "        output = {\n",
    "            'out': x\n",
    "        }\n",
    "\n",
    "        return output\n",
    "    \n",
    " \n",
    "# this section is only useful if we want to use a combined version of S1 and S2 data\n",
    "class DoubleSwinTransformerSegmentation(nn.Module):\n",
    "    def __init__(self, encoder1, encoder2, out_dim, device, freeze_layers=False):\n",
    "        super(DoubleSwinTransformerSegmentation, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.backbone1 = encoder1\n",
    "        self.backbone2 = encoder2\n",
    "\n",
    "        self.decoder1 = SwinTransformerDecoder(self.backbone1, out_dim, device)\n",
    "        self.decoder2 = SwinTransformerDecoder(self.backbone2, out_dim, device)\n",
    "\n",
    "        # freeze all backbone layers\n",
    "        if freeze_layers:\n",
    "            for name, param in self.named_parameters():\n",
    "                if name.startswith(('backbone')):\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, x2, x_seg2 = self.backbone2.forward_features(x[\"s2\"].to(self.device))\n",
    "        _, x1, x_seg1 = self.backbone1.forward_features(x[\"s1\"].to(self.device))\n",
    "\n",
    "        x1 = self.decoder1.forward_up_features(x1, x_seg1)\n",
    "        x2 = self.decoder2.forward_up_features(x2, x_seg2)\n",
    "\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "\n",
    "        output = self.decoder1.up_x4(x)\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57813df6-41a6-494a-84d8-9b00680b6a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.000014Z",
     "iopub.status.busy": "2022-12-09T20:39:49.999356Z",
     "iopub.status.idle": "2022-12-09T20:39:50.598008Z",
     "shell.execute_reply": "2022-12-09T20:39:50.597117Z",
     "shell.execute_reply.started": "2022-12-09T20:39:49.999936Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 12: Set model <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "# If want to use S2, replace s1_backbone with s2_backbone\n",
    "# If want to freeze layers of Transformer and only train segmentation head, set freeze_layers to True and make sure train_config['fine_tuning'] is set to False\n",
    "\n",
    "\n",
    "model = SingleSwinTransformerSegmentation(\n",
    "    s1_backbone, out_dim=2, device=device,\n",
    "    freeze_layers=True\n",
    ")\n",
    "\n",
    "net = model \n",
    "net = net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80eb90ab-c3f5-4b87-bf2b-4c7ab88c797f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.599486Z",
     "iopub.status.busy": "2022-12-09T20:39:50.599203Z",
     "iopub.status.idle": "2022-12-09T20:39:50.611293Z",
     "shell.execute_reply": "2022-12-09T20:39:50.609479Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.599446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen backbone\n"
     ]
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 13: Set model parameters <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Specific to Transformer\n",
    "\n",
    "if train_config['finetuning']:\n",
    "    # train all parameters (backbone + classifier head)\n",
    "    param_backbone = []\n",
    "    param_head = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            param_head.append(p)\n",
    "        else:\n",
    "            param_backbone.append(p)\n",
    "        p.requires_grad = True\n",
    "    # parameters = model.parameters()\n",
    "    parameters = [\n",
    "        {\"params\": param_backbone},  # train with default lr\n",
    "        {\n",
    "            \"params\": param_head,\n",
    "            \"lr\": train_config['classifier_lr'],\n",
    "        },  # train with classifier lr\n",
    "    ]\n",
    "    print(\"Finetuning\")\n",
    "\n",
    "else:\n",
    "    # train only final linear layer for SSL methods\n",
    "    print(\"Frozen backbone\")\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc1b9a7b-7c14-48c7-9868-b7d6b912866f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.612737Z",
     "iopub.status.busy": "2022-12-09T20:39:50.612359Z",
     "iopub.status.idle": "2022-12-09T20:39:50.619973Z",
     "shell.execute_reply": "2022-12-09T20:39:50.618070Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.612710Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 14: Set optimizer <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    parameters,\n",
    "    lr=train_config['learning_rate'],\n",
    "    betas=train_config['adam_betas'],\n",
    "    weight_decay=train_config['weight_decay'],\n",
    ")\n",
    "\n",
    "# optimizer = torch.optim.AdamW(parameters,lr=train_config['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45d3ed76-577b-4702-bfcb-b85ed96f6520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.621814Z",
     "iopub.status.busy": "2022-12-09T20:39:50.621461Z",
     "iopub.status.idle": "2022-12-09T20:39:50.636401Z",
     "shell.execute_reply": "2022-12-09T20:39:50.635237Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.621789Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 15: Set loss function & scheduler <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "#https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n",
    "'''\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target, smooth=1):\n",
    "        if output.shape[1] == 1:\n",
    "            output = fun.sigmoid(output)\n",
    "        else:\n",
    "            output = fun.softmax(output, dim=1)[:,1]\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        output = output.flatten()\n",
    "        target = target.flatten()\n",
    "        no_ignore = target.ne(255).cuda()\n",
    "        output = output.masked_select(no_ignore)\n",
    "        target = target.masked_select(no_ignore)\n",
    "        TP = torch.sum(output * target)\n",
    "        return 1 - ((2. * TP + smooth) / (output.sum() + target.sum() + smooth))\n",
    "'''\n",
    "\n",
    "\n",
    "class IOU(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IOU, self).__init__()\n",
    "\n",
    "    def forward(self, output, target, smooth=1):\n",
    "        if output.shape[1] == 1:\n",
    "            output = fun.sigmoid(output)\n",
    "        else:\n",
    "            output = fun.softmax(output, dim=1)[:,1]    \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        output = output.flatten()\n",
    "        target = target.flatten()\n",
    "        no_ignore = target.ne(255).cuda()\n",
    "        output = output.masked_select(no_ignore)\n",
    "        target = target.masked_select(no_ignore)\n",
    "        intersection = torch.sum(output * target)\n",
    "        union = torch.sum(target) + torch.sum(output) - intersection\n",
    "        return 1 - ((intersection + .0000001) / (union + .0000001))\n",
    "\n",
    "\n",
    "'''\n",
    "# Modification to Dice loss to correct class imbalance\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, smooth=1, alpha = .4, beta = .6, gamma = 1.5):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        if output.shape[1] == 1:\n",
    "            output = fun.sigmoid(output)\n",
    "        else:\n",
    "            output = fun.softmax(output, dim=1)[:,1]       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        output = output.flatten()\n",
    "        target = target.flatten()\n",
    "        no_ignore = target.ne(255).cuda()\n",
    "        output = output.masked_select(no_ignore)\n",
    "        target = target.masked_select(no_ignore)\n",
    "        TP = torch.sum(output * target)\n",
    "        FP = torch.sum((1 - target) * output)\n",
    "        FN = torch.sum(target * (1 - output))\n",
    "        return (1 - ((TP + self.smooth) / (TP + (self.alpha * FN) + (self.beta * FP) + self.smooth)))**self.gamma\n",
    "'''\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss(ignore_index=255, reduction=\"mean\").to(device)\n",
    "criterion = IOU()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, len(train_loader) * 10, T_mult=2, eta_min=0, last_epoch=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1687de0b-36cf-4c27-9d67-fe76ce67b484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.638766Z",
     "iopub.status.busy": "2022-12-09T20:39:50.638439Z",
     "iopub.status.idle": "2022-12-09T20:39:50.655978Z",
     "shell.execute_reply": "2022-12-09T20:39:50.654052Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.638739Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 16: Compute IOU Functions <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "def computeIOU(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  \n",
    "  no_ignore = target.ne(255).to(device)\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  intersection = torch.sum(output * target)\n",
    "  union = torch.sum(target) + torch.sum(output) - intersection\n",
    "  iou = (intersection + .0000001) / (union + .0000001)\n",
    "  \n",
    "  if iou != iou:\n",
    "    print(\"failed, replacing with 0\")\n",
    "    iou = torch.tensor(0).float()\n",
    "  \n",
    "  return iou\n",
    "  \n",
    "def computeAccuracy(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  \n",
    "  no_ignore = target.ne(255).to(device)\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  correct = torch.sum(output.eq(target))\n",
    "  \n",
    "  return correct.float() / len(target)\n",
    "\n",
    "def truePositives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).to(device)\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def trueNegatives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).to(device)\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 0)\n",
    "  target = (target == 0)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def falsePositives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 1)\n",
    "  target = (target == 0)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct\n",
    "\n",
    "def falseNegatives(output, target):\n",
    "  output = torch.argmax(output, dim=1).flatten() \n",
    "  target = target.flatten()\n",
    "  no_ignore = target.ne(255).cuda()\n",
    "  output = output.masked_select(no_ignore)\n",
    "  target = target.masked_select(no_ignore)\n",
    "  output = (output == 0)\n",
    "  target = (target == 1)\n",
    "  correct = torch.sum(output * target)\n",
    "  \n",
    "  return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "261b245d-bd57-4a90-9f3c-a36434866a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.658488Z",
     "iopub.status.busy": "2022-12-09T20:39:50.657944Z",
     "iopub.status.idle": "2022-12-09T20:39:50.669466Z",
     "shell.execute_reply": "2022-12-09T20:39:50.668136Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.658449Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 17: Define training loop <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "\n",
    "def train_loop(inputs, labels, net, optimizer, scheduler):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  \n",
    "  # zero the parameter gradients\n",
    "  optimizer.zero_grad()\n",
    "  net = net.to(device)\n",
    "  \n",
    "  # forward + backward + optimize\n",
    "  outputs = net(inputs.to(device))\n",
    "  loss = criterion(outputs[\"out\"], labels.long().to(device))\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  scheduler.step()\n",
    "\n",
    "  running_loss += loss\n",
    "  running_iou += computeIOU(outputs[\"out\"], labels.to(device))\n",
    "  running_accuracy += computeAccuracy(outputs[\"out\"], labels.to(device))\n",
    "  running_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abbaf573-c406-4145-ab58-2fc11dc64bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.671410Z",
     "iopub.status.busy": "2022-12-09T20:39:50.671096Z",
     "iopub.status.idle": "2022-12-09T20:39:50.690049Z",
     "shell.execute_reply": "2022-12-09T20:39:50.688579Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.671405Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 18: Define validation loop <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "from time import time\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "RUNNAME = 'test'\n",
    "\n",
    "def validation_loop(validation_data_loader, net):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  global max_valid_iou\n",
    "\n",
    "  global training_losses\n",
    "  global training_accuracies\n",
    "  global training_ious\n",
    "  global valid_losses\n",
    "  global valid_accuracies\n",
    "  global valid_ious\n",
    "\n",
    "  net = net.eval()\n",
    "  net = net.to(device)\n",
    "  count = 0\n",
    "  iou = 0\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "  with torch.no_grad():\n",
    "      for (images, labels) in validation_data_loader:\n",
    "          net = net.to(device)\n",
    "          outputs = net(images.to(device))\n",
    "          valid_loss = criterion(outputs[\"out\"], labels.long().to(device))\n",
    "          valid_iou = computeIOU(outputs[\"out\"], labels.to(device))\n",
    "          valid_accuracy = computeAccuracy(outputs[\"out\"], labels.to(device))\n",
    "          iou += valid_iou\n",
    "          loss += valid_loss\n",
    "          accuracy += valid_accuracy\n",
    "          count += 1\n",
    "\n",
    "  iou = iou / count\n",
    "  accuracy = accuracy / count\n",
    "\n",
    "  if iou > max_valid_iou:\n",
    "    pass\n",
    "    start = time()\n",
    "    max_valid_iou = iou\n",
    "    # save_path = os.path.join(\"checkpoints\", \"{}_{}_{}.cp\".format(RUNNAME, i, iou.item()))\n",
    "    save_path = os.path.join(\"chkpoints\", \"{}_{}_{}.cp\".format(RUNNAME, i, iou.item()))\n",
    "    # optim_save_path = os.path.join(\"checkpoints\", \"{}_{}_{}_{}.cp\".format(RUNNAME, i, iou.item(), 'optim'))\n",
    "    optim_save_path = os.path.join(\"chkpoints\", \"{}_{}_{}_{}.cp\".format(RUNNAME, i, iou.item(), 'optim'))\n",
    "    # scheduler_save_path = os.path.join(\"checkpoints\", \"{}_{}_{}_{}.cp\".format(RUNNAME, i, iou.item(), 'sheduler'))\n",
    "    scheduler_save_path = os.path.join(\"chkpoints\", \"{}_{}_{}_{}.cp\".format(RUNNAME, i, iou.item(), 'sheduler'))\n",
    "    torch.save(net.state_dict(), save_path)\n",
    "    torch.save(optimizer.state_dict(), optim_save_path)\n",
    "    torch.save(scheduler.state_dict(), scheduler_save_path)\n",
    "    print(\"model saved at\", save_path)\n",
    "    print('time:', time() - start)\n",
    "\n",
    "  loss = loss / count\n",
    "  print(\"Training Loss:\", running_loss / running_count)\n",
    "  print(\"Training IOU:\", running_iou / running_count)\n",
    "  print(\"Training Accuracy:\", running_accuracy / running_count)\n",
    "  print(\"Validation Loss:\", loss)\n",
    "  print(\"Validation IOU:\", iou)\n",
    "  print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "  \"\"\"training_losses.append(running_loss / running_count)\n",
    "  training_accuracies.append(running_accuracy / running_count)\n",
    "  training_ious.append(running_iou / running_count)\n",
    "  valid_losses.append(loss)\n",
    "  valid_accuracies.append(accuracy)\n",
    "  valid_ious.append(iou)\"\"\"\n",
    "  training_losses.append(running_loss.detach().cpu() / running_count)\n",
    "  training_accuracies.append(running_accuracy.detach().cpu() / running_count)\n",
    "  training_ious.append(running_iou.detach().cpu() / running_count)\n",
    "  valid_losses.append(loss.detach().cpu())\n",
    "  valid_accuracies.append(accuracy.detach().cpu())\n",
    "  valid_ious.append(iou.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a04409b-641c-4b55-8bb2-c391136dabf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.691789Z",
     "iopub.status.busy": "2022-12-09T20:39:50.691407Z",
     "iopub.status.idle": "2022-12-09T20:39:50.701048Z",
     "shell.execute_reply": "2022-12-09T20:39:50.699802Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.691745Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 19: Define test loop <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "def test_loop(test_data_loader, net):\n",
    "  net = net.eval()\n",
    "  net = net.to(device)\n",
    "  count = 0\n",
    "  iou = 0\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "  with torch.no_grad():\n",
    "      for (images, labels) in tqdm(test_data_loader):\n",
    "          net = net.to(device)\n",
    "          outputs = net(images.to(device))\n",
    "          valid_loss = criterion(outputs[\"out\"], labels.long().to(device))\n",
    "          valid_iou = computeIOU(outputs[\"out\"], labels.to(device))\n",
    "          iou += valid_iou\n",
    "          accuracy += computeAccuracy(outputs[\"out\"], labels.to(device))\n",
    "          count += 1\n",
    "\n",
    "  iou = iou / count\n",
    "  print(\"Test IOU:\", iou)\n",
    "  print(\"Test Accuracy:\", accuracy / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea934a57-313d-4464-b64f-67fc616589aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.702721Z",
     "iopub.status.busy": "2022-12-09T20:39:50.702365Z",
     "iopub.status.idle": "2022-12-09T20:39:50.733053Z",
     "shell.execute_reply": "2022-12-09T20:39:50.730909Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.702683Z"
    }
   },
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 20: Define train & validation scheme <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "running_loss = 0\n",
    "running_iou = 0\n",
    "running_count = 0\n",
    "running_accuracy = 0\n",
    "\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "\n",
    "def train_epoch(net, optimizer, scheduler, train_iter):\n",
    "  for (inputs, labels) in tqdm(train_iter):\n",
    "    train_loop(inputs.to(device), labels.to(device), net.to(device), optimizer, scheduler)\n",
    " \n",
    "\n",
    "def train_validation_loop(net, optimizer, scheduler, train_loader,\n",
    "                          valid_loader, num_epochs, cur_epoch):\n",
    "  global running_loss\n",
    "  global running_iou\n",
    "  global running_count\n",
    "  global running_accuracy\n",
    "  net = net.train()\n",
    "  running_loss = 0\n",
    "  running_iou = 0\n",
    "  running_count = 0\n",
    "  running_accuracy = 0\n",
    "  \n",
    "  for i in tqdm(range(num_epochs)):\n",
    "    train_iter = iter(train_loader)\n",
    "    train_epoch(net, optimizer, scheduler, train_iter)\n",
    "  clear_output()\n",
    "  \n",
    "  print(\"Current Epoch:\", cur_epoch)\n",
    "  validation_loop(iter(valid_loader), net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b77d0c30-91e2-4061-a600-58458c51b43a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.735304Z",
     "iopub.status.busy": "2022-12-09T20:39:50.734819Z",
     "iopub.status.idle": "2022-12-09T20:39:50.743729Z",
     "shell.execute_reply": "2022-12-09T20:39:50.742454Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.735292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff59739f-a553-4cfd-b195-4da3db5e0925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.745604Z",
     "iopub.status.busy": "2022-12-09T20:39:50.745306Z",
     "iopub.status.idle": "2022-12-09T20:39:50.752657Z",
     "shell.execute_reply": "2022-12-09T20:39:50.751549Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.745577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage\n"
     ]
    }
   ],
   "source": [
    "cd storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3306c7f1-cbf2-487b-9acd-67adb1bf0040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:50.754755Z",
     "iopub.status.busy": "2022-12-09T20:39:50.754284Z",
     "iopub.status.idle": "2022-12-09T20:39:51.590610Z",
     "shell.execute_reply": "2022-12-09T20:39:51.588999Z",
     "shell.execute_reply.started": "2022-12-09T20:39:50.754709Z"
    }
   },
   "outputs": [],
   "source": [
    "!sudo mkdir chkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7b739-69f8-481f-b0cf-93c6c7071b6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-09T20:39:51.592896Z",
     "iopub.status.busy": "2022-12-09T20:39:51.592514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch: 3\n",
      "Training Loss: tensor(0.5158, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training IOU: tensor(0.4842, device='cuda:0')\n",
      "Training Accuracy: tensor(0.9456, device='cuda:0')\n",
      "Validation Loss: tensor(0.5416, device='cuda:0')\n",
      "Validation IOU: tensor(0.4584, device='cuda:0')\n",
      "Validation Accuracy: tensor(0.9354, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max valid iou: tensor(0.5001, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e918037f86c4451fb0c43d4bf2ac7dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce78db840a942808d859053294376e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>> SECTION 21: Train model & assess metrics <<<<<<<<<<<<<<<<<<<<<<< #\n",
    "# Not specific to Transformer\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_valid_iou = 0\n",
    "start = 0\n",
    "\n",
    "epochs = []\n",
    "training_losses = []\n",
    "training_accuracies = []\n",
    "training_ious = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "valid_ious = []\n",
    "\n",
    "\n",
    "for i in range(start, train_config['epochs']):\n",
    "  train_validation_loop(net, optimizer, scheduler, train_loader, valid_loader, 1, i)\n",
    "  epochs.append(i)\n",
    "  x = epochs\n",
    "  plt.plot(x, training_losses, label='training losses')\n",
    "  plt.plot(x, training_accuracies, 'tab:orange', label='training accuracy')\n",
    "  plt.plot(x, training_ious, 'tab:purple', label='training iou')\n",
    "  plt.plot(x, valid_losses, label='valid losses')\n",
    "  plt.plot(x, valid_accuracies, 'tab:red',label='valid accuracy')\n",
    "  plt.plot(x, valid_ious, 'tab:green',label='valid iou')\n",
    "  plt.legend(loc=\"upper left\")\n",
    "\n",
    "  display(plt.show())\n",
    "\n",
    "  print(\"max valid iou:\", max_valid_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace6eb6-6306-4558-8d4e-633899009e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r chkpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
